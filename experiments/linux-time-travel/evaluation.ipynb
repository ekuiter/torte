{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pip install plotly pandas statsmodels kaleido scipy nbformat jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read CSV data\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os.path\n",
    "import pickle\n",
    "import scipy\n",
    "from statistics import mean, stdev\n",
    "from math import sqrt, log10\n",
    "from packaging.version import Version\n",
    "\n",
    "output_directory = '../../output-archive/output-linux-2024-11-08'\n",
    "figures_directory = '../../../paper-icse-2026-time-travel/images'\n",
    "default_height = 270\n",
    "\n",
    "pio.templates['colorblind'] = go.layout.Template(layout_colorway=['#648FFF', '#FE6100', '#785EF0', '#DC267F', '#FFB000'])\n",
    "pio.templates.default = 'plotly_white+colorblind'\n",
    "\n",
    "def read_dataframe(stage, dtype={}, usecols=None, file=None):\n",
    "    if not file:\n",
    "        file = 'output'\n",
    "    df = pd.read_csv(f'{output_directory}/{stage}/{file}.csv', dtype=dtype, usecols=usecols)\n",
    "    if 'committer_date_unix' in df:\n",
    "        df['committer_date'] = df['committer_date_unix'].apply(lambda d: pd.to_datetime(d, unit='s'))\n",
    "    return df\n",
    "\n",
    "def replace_values(df):\n",
    "    df.replace('kconfigreader', 'KConfigReader', inplace=True)\n",
    "    df.replace('kmax', 'KClause', inplace=True)\n",
    "\n",
    "def big_log10(str):\n",
    "    return log10(int(str)) if not pd.isna(str) and str != '' else pd.NA\n",
    "\n",
    "def process_model_count(df_solve):\n",
    "    df_solve['model-count'] = df_solve['model-count'].replace('1', '')\n",
    "    df_solve['model-count-log10'] = df_solve['model-count'].fillna('').apply(big_log10).replace(0, np.nan)\n",
    "    df_solve['year'] = df_solve['committer_date'].apply(lambda d: int(d.year))\n",
    "\n",
    "def peek_dataframe(df, column, message, type='str', filter=['revision', 'architecture', 'extractor']):\n",
    "    success = df[~df[column].str.contains('NA') if type == 'str' else ~df[column].isna()][filter]\n",
    "    failure = df[df[column].str.contains('NA') if type == 'str' else df[column].isna()][filter]\n",
    "    print(f'{message}: {len(success)} successes, {len(failure)} failures')\n",
    "\n",
    "df_architectures = read_dataframe(f'read-linux-architectures')\n",
    "df_architectures = df_architectures.sort_values(by='committer_date')\n",
    "df_architectures['year'] = df_architectures['committer_date'].apply(lambda d: int(d.year))\n",
    "\n",
    "df_configs = read_dataframe(f'read-linux-configs')\n",
    "df_configs = df_configs[~df_configs['kconfig-file'].str.contains('/um/')]\n",
    "\n",
    "df_config_types = read_dataframe(f'read-linux-configs', file='output.types')\n",
    "df_config_types = df_config_types[~df_config_types['kconfig-file'].str.contains('/um/')]\n",
    "df_config_types = df_config_types.merge(df_architectures[['revision', 'committer_date']].drop_duplicates())\n",
    "\n",
    "df_kconfig = read_dataframe('kconfig')\n",
    "df_kconfig['year'] = df_kconfig['committer_date'].apply(lambda d: int(d.year))\n",
    "\n",
    "df_uvl = read_dataframe('model_to_uvl_featureide')\n",
    "df_smt = read_dataframe('model_to_smt_z3')\n",
    "df_dimacs = read_dataframe('dimacs')\n",
    "df_backbone_dimacs = read_dataframe('backbone-dimacs')\n",
    "\n",
    "df_solve = read_dataframe('solve_model-count', {'model-count': 'string'})\n",
    "process_model_count(df_solve)\n",
    "\n",
    "if os.path.isfile(f'{output_directory}/model-count-with-6h-timeout.csv'):\n",
    "    df_solve_6h = pd.read_csv(f'{output_directory}/model-count-with-6h-timeout.csv', dtype={'model-count': 'string'})\n",
    "    df_solve_6h = df_backbone_dimacs.merge(df_solve_6h)\n",
    "    process_model_count(df_solve_6h)\n",
    "    df_solve = pd.merge(df_solve, df_solve_6h[['revision','architecture', 'extractor', 'backbone.dimacs-analyzer']], indicator=True, how='outer') \\\n",
    "        .query('_merge==\"left_only\"') \\\n",
    "        .drop('_merge', axis=1)\n",
    "    df_solve = pd.concat([df_solve, df_solve_6h])\n",
    "else:\n",
    "    df_solve_6h = None\n",
    "\n",
    "for df in [df_kconfig, df_uvl, df_smt, df_dimacs, df_backbone_dimacs, df_solve]:\n",
    "    replace_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for drawing plots\n",
    "\n",
    "def estimate_group(group):\n",
    "    print('\\\\hspace{2mm} ' + group + ' \\\\\\\\')\n",
    "\n",
    "def estimate_trend(fig, color=None, color_value=None, xs=[], key=lambda x: x.timestamp()):\n",
    "    results = px.get_trendline_results(fig)\n",
    "    if color is not None and color_value is not None:\n",
    "        idx = [i for i, r in enumerate(results.iloc) if r[color] == color_value][0]\n",
    "    else:\n",
    "        idx = 0\n",
    "    intercept = results.iloc[idx]['px_fit_results'].params[0]\n",
    "    slope = results.iloc[idx]['px_fit_results'].params[1]\n",
    "    daily = slope * pd.to_timedelta(1, unit='D').total_seconds()\n",
    "    weekly = slope * pd.to_timedelta(7, unit='D').total_seconds()\n",
    "    monthly = slope * pd.to_timedelta(1, unit='D').total_seconds() * 30.437\n",
    "    yearly = slope * pd.to_timedelta(1, unit='D').total_seconds() * 365.25\n",
    "    return daily, weekly, monthly, yearly, [intercept + slope * key(x) for x in xs]\n",
    "\n",
    "def committer_date_x_axis(fig, df=df_kconfig, append_revision=True, step=1):\n",
    "    axis = df_kconfig[['committer_date', 'revision']].drop_duplicates()\n",
    "    axis['year'] = axis['committer_date'].apply(lambda d: str(d.year))\n",
    "    axis = axis.sort_values(by='committer_date').groupby('year').nth(0).reset_index()\n",
    "    fig.update_xaxes(\n",
    "        ticktext=axis['year'].str.cat('<br><sup>' + axis['revision'].str[1:] + '</sup>')[1::step] if append_revision else axis['year'][::step],\n",
    "        tickvals=axis['year'][1::step]\n",
    "    )\n",
    "\n",
    "def revision_x_axis(fig, df=df_kconfig):\n",
    "    axis = df_kconfig[['committer_date', 'revision']].drop_duplicates()\n",
    "    axis['year'] = axis['committer_date'].apply(lambda d: str(d.year))\n",
    "    axis = axis.sort_values(by='committer_date').groupby('year').nth(0).reset_index()\n",
    "    fig.update_xaxes(\n",
    "        ticktext=axis['year'],\n",
    "        tickvals=axis['revision']\n",
    "    )\n",
    "\n",
    "def log10_y_axis(fig):\n",
    "    fig.update_yaxes(tickprefix = '10<sup>', ticksuffix = '</sup>')\n",
    "\n",
    "def percentage_y_axis(fig):\n",
    "    fig.layout.yaxis.tickformat = ',.0%'\n",
    "\n",
    "def format_percentage(value):\n",
    "    return str(round(value * 100, 2)) + '%'\n",
    "\n",
    "def committer_date_labels(dict={}):\n",
    "    return {'committer_date': 'Year<br><sup>First Release in Year</sup>'} | dict\n",
    "\n",
    "def revision_labels(dict={}):\n",
    "    return {'revision': 'Year'} | dict\n",
    "\n",
    "def style_legend(fig, position='topleft', xshift=0, yshift=0):\n",
    "    if position == 'topleft':\n",
    "        fig.update_layout(legend=dict(yanchor='top', y=0.98 + yshift, xanchor='left', x=0.01 + xshift))\n",
    "    elif position == 'topright':\n",
    "        fig.update_layout(legend=dict(yanchor='top', y=0.98 + yshift, xanchor='right', x=0.98 + xshift))\n",
    "    elif position == 'bottomright':\n",
    "        fig.update_layout(legend=dict(yanchor='bottom', y=0.01 + yshift, xanchor='right', x=0.98 + xshift))\n",
    "    elif position == 'bottomleft':\n",
    "        fig.update_layout(legend=dict(yanchor='bottom', y=0.01 + yshift, xanchor='left', x=0.01 + xshift))\n",
    "    elif position == 'right':\n",
    "        return\n",
    "    elif position == 'horizontal':\n",
    "        fig.update_layout(legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1))\n",
    "    else:\n",
    "        fig.update_layout(showlegend=False)\n",
    "\n",
    "def style_box(fig, legend_position='topleft', xshift=0, yshift=0):\n",
    "    fig.update_traces(fillcolor='rgba(0,0,0,0)')\n",
    "    fig.update_traces(line_width=1)\n",
    "    fig.update_traces(marker_size=2)\n",
    "    fig.update_layout(font_family=\"Linux Biolinum\")\n",
    "    style_legend(fig, legend_position, xshift, yshift)\n",
    "\n",
    "def style_scatter(fig, marker_size=4, legend_position='topleft', xshift=0, yshift=0):\n",
    "    if marker_size:\n",
    "        fig.update_traces(marker_size=marker_size)\n",
    "    style_legend(fig, legend_position, xshift, yshift)\n",
    "    fig.update_layout(font_family=\"Linux Biolinum\")\n",
    "\n",
    "def plot_failures(fig, df, x, y, y_value, align='bottom', xref='x', font_size=10, textangle=270):\n",
    "    group = df.groupby(x, dropna=False)\n",
    "    failures = (group[y].size() - group[y].count()).reset_index().rename(columns={y: f'{y}_failures'})\n",
    "    attempts = group[y].size().reset_index().rename(columns={y: f'{y}_attempts'})\n",
    "    failures = pd.merge(failures, attempts)\n",
    "    failures[f'{y}_text'] = failures[f'{y}_failures'].astype(str) + ' (' + (failures[f'{y}_failures'] / failures[f'{y}_attempts']).apply(lambda v: \"{0:.1f}%\".format(v * 100)) + ')'\n",
    "    for row in range(len(failures)):\n",
    "        text = failures.at[row, f'{y}_text']\n",
    "        text = \"\" if failures.at[row, f'{y}_failures'] == 0 else text\n",
    "        fig.add_annotation(\n",
    "            x=failures.at[row, x],\n",
    "            y=y_value,\n",
    "            text=text,\n",
    "            showarrow=False,\n",
    "            font_size=font_size,\n",
    "            textangle=textangle,\n",
    "            align='left' if align == 'bottom' else 'right',\n",
    "            yanchor='bottom' if align == 'bottom' else 'top',\n",
    "            yshift=5 if align == 'bottom' else -5,\n",
    "            font_color='gray',\n",
    "            xref=xref\n",
    "        )\n",
    "\n",
    "def cohens_d(d1, d2):\n",
    "    # uses pooled standard deviation\n",
    "    n1, n2 = len(d1), len(d2)\n",
    "    s1, s2 = np.var(d1, ddof=1), np.var(d2, ddof=1)\n",
    "    s = np.sqrt(((n1 - 1) * s1 + (n2 - 1) * s2) / (n1 + n2 - 2))\n",
    "    u1, u2 = np.mean(d1), np.mean(d2)\n",
    "    return (u1 - u2) / s\n",
    "\n",
    "def effect_size_mannwhitneyu(d1, d2):\n",
    "    u = scipy.stats.mannwhitneyu(d1, d2).statistic\n",
    "    n1, n2 = len(d1), len(d2)\n",
    "    n = n1 + n2\n",
    "    mean_U = n1 * n2 / 2\n",
    "    std_U = np.sqrt(n1 * n2 * (n + 1) / 12)\n",
    "    z = (u - mean_U) / std_U\n",
    "    r = z / np.sqrt(n)\n",
    "    return r\n",
    "\n",
    "def wilcoxon_test(df, column_a, column_b):\n",
    "    # if the same values are returned for many inputs, refer to https://stats.stackexchange.com/q/232927\n",
    "    a = df[column_a][~df[column_a].isna()]\n",
    "    b = df[column_b][~df[column_b].isna()]\n",
    "    d = a - b\n",
    "    results = scipy.stats.wilcoxon(d, method='approx')\n",
    "    p = results.pvalue\n",
    "    # adapted from https://stats.stackexchange.com/q/133077\n",
    "    r = np.abs(results.zstatistic / np.sqrt(len(d) * 2))\n",
    "    return p, r\n",
    "\n",
    "def style_p_values(fig, brackets, scale=0, _format=dict(interline=0.07, text_height=1.07, color='gray')):\n",
    "    # adapted from https://stackoverflow.com/q/67505252\n",
    "    for entry in brackets:\n",
    "        first_column, second_column, y, results = entry\n",
    "        y_range = [1.01+y*_format['interline'], 1.02+y*_format['interline']]\n",
    "        p, r = results\n",
    "        if p >= 0.05:\n",
    "            symbol = 'ns'\n",
    "        elif p >= 0.01: \n",
    "            symbol = '*'\n",
    "        elif p >= 0.001:\n",
    "            symbol = '**'\n",
    "        else:\n",
    "            symbol = '***'\n",
    "        first_column = first_column - scale\n",
    "        second_column = second_column + scale\n",
    "        fig.add_shape(type=\"line\",\n",
    "            xref=\"x\", yref=\"y domain\",\n",
    "            x0=first_column, y0=y_range[0],\n",
    "            x1=first_column, y1=y_range[1],\n",
    "            line=dict(color=_format['color'], width=2,)\n",
    "        )\n",
    "        fig.add_shape(type=\"line\",\n",
    "            xref=\"x\", yref=\"y domain\",\n",
    "            x0=first_column, y0=y_range[1], \n",
    "            x1=second_column, y1=y_range[1],\n",
    "            line=dict(color=_format['color'], width=2,)\n",
    "        )\n",
    "        fig.add_shape(type=\"line\",\n",
    "            xref=\"x\", yref=\"y domain\",\n",
    "            x0=second_column, y0=y_range[0], \n",
    "            x1=second_column, y1=y_range[1],\n",
    "            line=dict(color=_format['color'], width=2,)\n",
    "        )\n",
    "        fig.add_annotation(dict(font=dict(color=_format['color'],size=14),\n",
    "            x=(first_column + second_column)/2,\n",
    "            y=y_range[1]*_format['text_height'],\n",
    "            showarrow=False,\n",
    "            text=symbol + ' <sup>(' + str(round(r, 2)) + ')</sup>',\n",
    "            textangle=0,\n",
    "            xref=\"x\",\n",
    "            yref=\"y domain\"\n",
    "        ))\n",
    "    return fig\n",
    "\n",
    "def bracket_for(i, j, xshift, y, results):\n",
    "    return [i + xshift, j + xshift, y, results]\n",
    "\n",
    "def filter_extractor(df, extractor):\n",
    "    return df[df['extractor'] == extractor]\n",
    "\n",
    "def annotate_value(fig, x, y, subplot, prefix, ax, ay, xanchor, df, fn=lambda prefix, label: prefix + ': ' + label if label else prefix, label=None, size=None):\n",
    "    if isinstance(x, str):\n",
    "        x = df[x].iat[0]\n",
    "    if isinstance(y, str):\n",
    "        y = log10(df[y].iat[0]/1000000000)\n",
    "    if isinstance(label, str):\n",
    "        label = df[label].iat[0]\n",
    "    else:\n",
    "        label = format(round(y), ',')  if y > 0 else None\n",
    "    fig.add_annotation(\n",
    "        xref='x' + str(subplot),\n",
    "        yref='y' + str(subplot),\n",
    "        x=x,\n",
    "        y=y,\n",
    "        ax=ax,\n",
    "        ay=ay,\n",
    "        xanchor=xanchor,\n",
    "        text=fn(prefix, label),\n",
    "        font=dict(size=size)\n",
    "    )\n",
    "\n",
    "def show(fig, name=None, width=1000, height=500, margin=None, show=True, format='pdf', scale=1):\n",
    "    fig.update_layout(width=width, height=height)\n",
    "    if margin:\n",
    "        fig.update_layout(margin=margin)\n",
    "    else:\n",
    "        fig.update_layout(margin=dict(l=0, r=0, t=0, b=0))\n",
    "    if figures_directory and os.path.isdir(figures_directory) and name:\n",
    "        fig.write_image(f'{figures_directory}/{name}.{format}', scale=scale)\n",
    "    if show:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differentiate kinds of features\n",
    "\n",
    "potential_misses_grep = set()\n",
    "potential_misses_kmax = set()\n",
    "extractor_comparison = {}\n",
    "df_configs_configurable = df_configs.copy()\n",
    "df_configs_configurable['configurable'] = False\n",
    "\n",
    "def jaccard(a, b):\n",
    "    return len(set.intersection(a, b)) / len(set.union(a, b))\n",
    "\n",
    "def add_features(descriptor, source, features, min=2):\n",
    "    descriptor[f'#{source}'] = len(features) if features is not None and len(features) >= min else np.nan\n",
    "\n",
    "def get_variables(variable_map):\n",
    "    variables = set(variable_map.values())\n",
    "    if len(variables) <= 1:\n",
    "        variables = set()\n",
    "    return variables\n",
    "\n",
    "def read_unconstrained_feature_variables(extractor, revision, architecture):\n",
    "    unconstrained_features_filename = f'{output_directory}/unconstrained-features/{extractor}/linux/{revision}[{architecture}].unconstrained.features'\n",
    "    unconstrained_feature_variables = set()\n",
    "    if os.path.isfile(unconstrained_features_filename):\n",
    "        with open(unconstrained_features_filename, 'r') as f:\n",
    "            unconstrained_feature_variables = set([re.sub('^CONFIG_', '', f.strip()) for f in f.readlines()])\n",
    "    return unconstrained_feature_variables\n",
    "\n",
    "def inspect_architecture_features_for_model(extractor, revision, architecture, config_features, features_for_last_revision):\n",
    "    global potential_misses_grep, potential_misses_kmax\n",
    "    \n",
    "    features_filename = f'{output_directory}/kconfig/{extractor}/linux/{revision}[{architecture}].features'\n",
    "    with open(features_filename, 'r') as f:\n",
    "        extracted_features = set([re.sub('^CONFIG_', '', f.strip()) for f in f.readlines()])\n",
    "    \n",
    "    unconstrained_feature_variables = read_unconstrained_feature_variables(extractor, revision, architecture)\n",
    "\n",
    "    dimacs_filename = f'{output_directory}/backbone-dimacs/{extractor}/linux/{revision}[{architecture}].backbone.dimacs'\n",
    "    all_variables = set()\n",
    "    variables = set()\n",
    "    feature_variables = set()\n",
    "    core_feature_variables = set()\n",
    "    dead_feature_variables = set()\n",
    "    undead_feature_variables = set()\n",
    "    all_feature_variables = set()\n",
    "    features = set()\n",
    "    core_features = set()\n",
    "    unconstrained_features = set()\n",
    "    constrained_features = set()\n",
    "    added_features = None\n",
    "    removed_features = None\n",
    "    infos = {'extracted_features_jaccard': np.nan, \\\n",
    "                     'all_variables_jaccard': np.nan, \\\n",
    "                     'variables_jaccard': np.nan, \\\n",
    "                     'feature_variables_jaccard': np.nan, \\\n",
    "                     'undead_feature_variables_jaccard': np.nan, \\\n",
    "                     'all_feature_variables_jaccard': np.nan, \\\n",
    "                     'features_jaccard': np.nan, \\\n",
    "                     'unconstrained_bools': np.nan, \\\n",
    "                     'unconstrained_tristates': np.nan}\n",
    "    \n",
    "    if os.path.isfile(dimacs_filename):\n",
    "        with open(dimacs_filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            all_variable_map = {}\n",
    "            variable_map = {}\n",
    "            feature_variable_map = {}\n",
    "            for f in lines:\n",
    "                if f.startswith('c '):\n",
    "                    result = re.search('^c ([^ ]+) ([^ ]+)$', f)\n",
    "                    if result:\n",
    "                        index = int(result.group(1).strip())\n",
    "                        name = result.group(2).strip()\n",
    "                        all_variable_map[index] = name\n",
    "                        if \"k!\" not in name:\n",
    "                            variable_map[index] = name\n",
    "                            if name != 'True' \\\n",
    "                                and name != '<unsupported>' \\\n",
    "                                and name != 'PREDICATE_Compare' \\\n",
    "                                and not name.startswith('__VISIBILITY__CONFIG_') \\\n",
    "                                and not name.endswith('_MODULE'):\n",
    "                                feature_variable_map[index] = name\n",
    "            all_variables = get_variables(all_variable_map)\n",
    "            variables = get_variables(variable_map)\n",
    "            feature_variables = get_variables(feature_variable_map)\n",
    "\n",
    "            backbone_features_filename = f'{output_directory}/backbone-features/{extractor}/linux/{revision}[{architecture}].backbone.features'\n",
    "            if os.path.isfile(backbone_features_filename):\n",
    "                with open(backbone_features_filename, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    if len(lines) > 1:\n",
    "                        core_feature_variables = set([line[1:].strip() for line in lines if line.startswith('+')]).intersection(feature_variables)\n",
    "                        dead_feature_variables = set([line[1:].strip() for line in lines if line.startswith('-')]).intersection(feature_variables)\n",
    "\n",
    "            if len(feature_variables) > 0:\n",
    "                undead_feature_variables = feature_variables.difference(dead_feature_variables)\n",
    "                all_feature_variables = undead_feature_variables.union(unconstrained_feature_variables)\n",
    "                features = all_feature_variables.intersection(config_features)\n",
    "                if f'{revision}###{architecture}' not in extractor_comparison:\n",
    "                    extractor_comparison[f'{revision}###{architecture}'] = features\n",
    "                else:\n",
    "                    extractor_comparison[f'{revision}###{architecture}'] = jaccard(extractor_comparison[f'{revision}###{architecture}'], features)\n",
    "                core_features = features.intersection(core_feature_variables)\n",
    "                unconstrained_features = features.intersection(unconstrained_feature_variables)\n",
    "                unconstrained_features_by_type = pd.DataFrame(list(unconstrained_features), columns=['config']) \\\n",
    "                    .merge(df_config_types[(df_config_types['revision'] == revision)])\n",
    "                unconstrained_bools = unconstrained_features_by_type[unconstrained_features_by_type['type'] == 'bool']['config'].drop_duplicates()\n",
    "                unconstrained_tristates = unconstrained_features_by_type[unconstrained_features_by_type['type'] == 'tristate']['config'].drop_duplicates()\n",
    "                constrained_features = features.difference(core_feature_variables).difference(unconstrained_feature_variables)\n",
    "                if architecture in features_for_last_revision and len(features_for_last_revision[architecture]) > 0:\n",
    "                    added_features = features.difference(features_for_last_revision[architecture])\n",
    "                    removed_features = features_for_last_revision[architecture].difference(features)\n",
    "                infos = { \\\n",
    "                            'extracted_features_jaccard': jaccard(extracted_features, features), \\\n",
    "                            'all_variables_jaccard': jaccard(all_variables, features), \\\n",
    "                            'variables_jaccard': jaccard(variables, features), \\\n",
    "                            'feature_variables_jaccard': jaccard(feature_variables, features), \\\n",
    "                            'undead_feature_variables_jaccard': jaccard(undead_feature_variables, features), \\\n",
    "                            'all_feature_variables_jaccard': jaccard(all_feature_variables, features), \\\n",
    "                            'features_jaccard': 1, \\\n",
    "                            'unconstrained_bools': len(unconstrained_bools), \\\n",
    "                            'unconstrained_tristates': len(unconstrained_tristates) \\\n",
    "                        }\n",
    "    descriptor = {'extractor': extractor, 'revision': revision, 'architecture': architecture} | infos\n",
    "    add_features(descriptor, 'config_features', config_features) # F_config\n",
    "    add_features(descriptor, 'extracted_features', extracted_features) # F_extracted\n",
    "    add_features(descriptor, 'unconstrained_feature_variables', unconstrained_feature_variables, min=1) # F_unconstrained\n",
    "    add_features(descriptor, 'all_variables', all_variables) # V_all\n",
    "    add_features(descriptor, 'variables', variables) # V_phi\n",
    "    add_features(descriptor, 'feature_variables', feature_variables) # FV_phi\n",
    "    add_features(descriptor, 'core_feature_variables', core_feature_variables, min=1) # FV_core\n",
    "    add_features(descriptor, 'dead_feature_variables', dead_feature_variables, min=1) # FV_dead\n",
    "    add_features(descriptor, 'constrained_feature_variables', undead_feature_variables.difference(core_feature_variables)) # FV_constrained\n",
    "    add_features(descriptor, 'undead_feature_variables', undead_feature_variables) # FV_undead\n",
    "    add_features(descriptor, 'all_feature_variables', all_feature_variables) # FV\n",
    "    add_features(descriptor, 'ALL_feature_variables', feature_variables.union(unconstrained_feature_variables)) # FV_all\n",
    "    add_features(descriptor, 'features', features) # F\n",
    "    add_features(descriptor, 'core_features', core_features, min=1)\n",
    "    add_features(descriptor, 'unconstrained_features', unconstrained_features, min=1)\n",
    "    add_features(descriptor, 'constrained_features', constrained_features)\n",
    "    add_features(descriptor, 'added_features', added_features, min=0)\n",
    "    add_features(descriptor, 'removed_features', removed_features, min=0)\n",
    "    if extractor == 'kmax':\n",
    "        potential_misses_grep.update([f for f in all_feature_variables.difference(features) if '__CONFIG_' not in f])\n",
    "    return descriptor, feature_variables.union(unconstrained_feature_variables), features\n",
    "\n",
    "def inspect_architecture_features_for_revision(extractor, revision, features_for_last_revision):\n",
    "    config_features = set(df_configs[df_configs['revision'] == revision]['config'])\n",
    "    architectures = [re.search('\\[(.*)\\]', f).group(1) for f in glob.glob(f'{output_directory}/kconfig/{extractor}/linux/{revision}[*.features')]\n",
    "    architectures = list(set(architectures))\n",
    "    architectures.sort()\n",
    "    data = []\n",
    "    total_features = set()\n",
    "    total_feature_variables = set()\n",
    "    features_for_current_revision = {}\n",
    "    for architecture in architectures:\n",
    "        descriptor, feature_variables, features = inspect_architecture_features_for_model(extractor, revision, architecture, config_features, features_for_last_revision)\n",
    "        data.append(descriptor)\n",
    "        total_features.update(features)\n",
    "        features_for_current_revision[architecture] = features\n",
    "        if extractor == 'kmax':\n",
    "            total_feature_variables.update(feature_variables)\n",
    "    for descriptor in data:\n",
    "        add_features(descriptor, 'total_features', total_features)\n",
    "        total_added_features = None\n",
    "        total_removed_features = None\n",
    "        if 'TOTAL' in features_for_last_revision and len(features_for_last_revision['TOTAL']) > 0:\n",
    "            total_added_features = total_features.difference(features_for_last_revision['TOTAL'])\n",
    "            total_removed_features = features_for_last_revision['TOTAL'].difference(total_features)\n",
    "        add_features(descriptor, 'total_added_features', total_added_features, min=0)\n",
    "        add_features(descriptor, 'total_removed_features', total_removed_features, min=0)\n",
    "    features_for_current_revision['TOTAL'] = total_features\n",
    "    df_configs_configurable.loc[(df_configs_configurable['revision'] == revision) & (df_configs_configurable['config'].isin(total_features)), 'configurable'] = True\n",
    "    if extractor == 'kmax':\n",
    "        potential_misses_kmax.update([f for f in config_features.difference(total_feature_variables)])\n",
    "    return data, features_for_current_revision\n",
    "\n",
    "def inspect_architecture_features(extractor):\n",
    "    print(f'{extractor} ', end='')\n",
    "    revisions = [re.search('/linux/(.*)\\[', f).group(1) for f in glob.glob(f'{output_directory}/kconfig/{extractor}/linux/*.features')]\n",
    "    revisions = list(set(revisions))\n",
    "    revisions.sort(key=Version)\n",
    "    data = []\n",
    "    features_for_last_revision = {}\n",
    "    i = 0\n",
    "    for revision in revisions:\n",
    "        i += 1\n",
    "        if i % 10 == 0:\n",
    "            print(revision + ' . ', end='')\n",
    "        new_data, features_for_last_revision = inspect_architecture_features_for_revision(extractor, revision, features_for_last_revision)\n",
    "        data += new_data\n",
    "    print()\n",
    "    return data\n",
    "\n",
    "if os.path.isfile(f'{output_directory}/linux-features.dat'):\n",
    "    with open(f'{output_directory}/linux-features.dat', 'rb') as f:\n",
    "        [features_by_kind_per_architecture, df_extractor_comparison, potential_misses_grep, potential_misses_kmax, df_configs_configurable] = pickle.load(f)\n",
    "else:\n",
    "    features_by_kind_per_architecture = inspect_architecture_features('kconfigreader')\n",
    "    features_by_kind_per_architecture += inspect_architecture_features('kmax')\n",
    "    features_by_kind_per_architecture = pd.DataFrame(features_by_kind_per_architecture)\n",
    "    df_extractor_comparison = []\n",
    "    for key, value in extractor_comparison.items():\n",
    "        [revision, architecture] = key.split('###')\n",
    "        if type(value) is set:\n",
    "            value = pd.NA\n",
    "        df_extractor_comparison.append({'revision': revision, 'architecture': architecture, 'extractor_jaccard': value})\n",
    "    df_extractor_comparison = pd.DataFrame(df_extractor_comparison)\n",
    "    with open(f'{output_directory}/linux-features.dat', 'wb') as f:\n",
    "        pickle.dump([features_by_kind_per_architecture, df_extractor_comparison, potential_misses_grep, potential_misses_kmax, df_configs_configurable], f)\n",
    "\n",
    "replace_values(features_by_kind_per_architecture)\n",
    "df_features = pd.merge(df_architectures, features_by_kind_per_architecture, how='outer').sort_values(by='committer_date')\n",
    "df_features = pd.merge(df_kconfig, df_features, how='outer').sort_values(by='committer_date')\n",
    "\n",
    "def compare_with_grep(message, list):\n",
    "    print(f'{message}: ' + str(len(list)))\n",
    "    print(pd.merge(df_configs[['config','kconfig-file']], pd.DataFrame(list, columns=['config']), how='inner') \\\n",
    "        .drop_duplicates().merge(df_config_types[['config', 'type']]).drop_duplicates())\n",
    "\n",
    "def report_potential_misses(potential_misses_grep, potential_misses_kmax):\n",
    "    # these are the features NOT found by grep, but found by kmax (this allows us to check whether the grep regex matches too much)\n",
    "    # the only matches are enviroment variables (e.g., ARCH) and mistakes in kconfig files: IA64_SGI_UV (which has a trailing `) and SND_SOC_UX500_MACH_MOP500 (which has a leading +)\n",
    "    compare_with_grep('#potential misses (grep)', potential_misses_grep)\n",
    "    print()\n",
    "\n",
    "    # these are the features found by grep, but NOT found by kmax, either constrained or unconstrained (this allows us to check whether kmax matches enough)\n",
    "    # as there are some extraction failures for kmax, we expect some misses; also, we do not extract the um architecture; and finally, there are some test kconfig files that are never included\n",
    "    # in the following, we try to filter out these effects (this is not perfect though)\n",
    "    potential_misses_kmax_with_type = (pd.merge(df_configs[['config','kconfig-file', 'revision']], pd.DataFrame(potential_misses_kmax, columns=['config']), how='inner') \\\n",
    "            .drop_duplicates().merge(df_config_types[['config', 'type']]).drop_duplicates())\n",
    "    misses_due_to_tests = set(potential_misses_kmax_with_type[ \\\n",
    "            potential_misses_kmax_with_type['kconfig-file'].str.startswith('Documentation/') | \\\n",
    "            potential_misses_kmax_with_type['kconfig-file'].str.startswith('scripts/')]['config'].unique())\n",
    "    missing_kmax_models = df_features[(df_features['extractor'] == 'KClause') & df_features['#extracted_features'].isna()]\n",
    "    missing_kmax_models = missing_kmax_models[['revision', 'architecture']].drop_duplicates()\n",
    "    potential_misses_kmax_with_type['architecture'] = potential_misses_kmax_with_type['kconfig-file'].apply(lambda s: re.sub(r'^arch/(.*?)/.*$', r'\\1', s))\n",
    "    potential_misses_due_to_missing_kmax_models = set(potential_misses_kmax_with_type.merge(missing_kmax_models[['revision', 'architecture']].drop_duplicates()) \\\n",
    "                                                    .drop(columns=['kconfig-file', 'revision', 'architecture', 'type'])['config'].unique())\n",
    "    potential_misses_kmax = potential_misses_kmax.difference(misses_due_to_tests).difference(potential_misses_due_to_missing_kmax_models)\n",
    "    # the remaining matches are due to our way of using kmax extractor, where we ignore lines with new kconfig constructs like $(success,...)\n",
    "    compare_with_grep('#potential misses (kmax)', potential_misses_kmax)\n",
    "\n",
    "report_potential_misses(potential_misses_grep, potential_misses_kmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "df_total_features = df_features.groupby(['extractor', 'revision']).agg({'#total_features': 'min'}).reset_index()\n",
    "df_total_features = pd.merge(df_kconfig[['committer_date', 'revision']].drop_duplicates(), df_total_features)\n",
    "df_total_features = df_total_features[df_total_features['extractor']=='KClause']\n",
    "df = df_total_features.sort_values(by='committer_date')\n",
    "df = df[['committer_date', '#total_features']].drop_duplicates()\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df['committer_date'], y=df['#total_features'], mode='markers', name='Number of Features (r=.99)', marker_symbol='circle'),\n",
    "    secondary_y=False\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_kconfig['committer_date'], y=df_kconfig['source_lines_of_code'], mode='markers', name='Source Lines of Code (r=.98)', marker_symbol='circle'),\n",
    "    secondary_y=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Year\")\n",
    "fig.update_yaxes(title_text=\"Number of Features\", dtick=5000, range=[0, 22000], color=\"#648FFF\", title_font_color=\"black\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"Source Lines of Code\", dtick=10000000, range=[0, 32000000], color=\"#FE6100\", title_font_color=\"black\", secondary_y=True)\n",
    "\n",
    "style_scatter(fig, marker_size=3, legend_position='topleft')\n",
    "fig.update_xaxes(range=[\"2002-01-01\", \"2025-01-01\"])\n",
    "show(fig, 'sloc', width=400, height=260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics in section 1\n",
    "\n",
    "# show(px.scatter(\n",
    "#     df_kconfig,\n",
    "#     x='committer_date',\n",
    "#     y='source_lines_of_code',\n",
    "#     trendline='ols'\n",
    "# ))\n",
    "\n",
    "# show(px.scatter(\n",
    "#     df,\n",
    "#     x='committer_date',\n",
    "#     y='#total_features',\n",
    "#     trendline='ols'\n",
    "# ))\n",
    "\n",
    "# linear regression of source lines of code and number of features\n",
    "print(scipy.stats.pearsonr(df_kconfig['committer_date'].astype(int) // 10 ** 9, df_kconfig['source_lines_of_code']))\n",
    "print(scipy.stats.pearsonr(df['committer_date'].astype(int) // 10 ** 9, df['#total_features']))\n",
    "print(scipy.stats.linregress(df_kconfig['committer_date'].astype(int) // 10 ** 9, df_kconfig['source_lines_of_code']))\n",
    "print(scipy.stats.linregress(df['committer_date'].astype(int) // 10 ** 9, df['#total_features']))\n",
    "\n",
    "# lines of code per year\n",
    "print(0.032409366566693104*60*60*24*365.25)\n",
    "# features per year\n",
    "print(2.6766885381596744e-05*60*60*24*365.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year(d):\n",
    "    try:\n",
    "        return 2000 + int(d.split('-')[0])\n",
    "    except:\n",
    "        return int(d.split('-')[-1])\n",
    "\n",
    "def scope_to(df, source_descriptor):\n",
    "    if source_descriptor.endswith('==2024'):\n",
    "        df = df[df['year-analyzer']==2024]\n",
    "        source_descriptor = source_descriptor[:-len('==2024')]\n",
    "    if source_descriptor.endswith('!=2024'):\n",
    "        df = df[df['year-analyzer']!=2024]\n",
    "        source_descriptor = source_descriptor[:-len('!=2024')]\n",
    "    if len(source_descriptor) > 0:\n",
    "        if source_descriptor == 'sat-museum+competition':\n",
    "            return df[(df['source-analyzer']=='sat-museum')|((df['source-analyzer']=='sat-competition')&(df['year-analyzer']>=2023))]\n",
    "        df = df[df['source-analyzer']==source_descriptor]\n",
    "    return df\n",
    "\n",
    "pd.set_option('mode.copy_on_write', True)\n",
    "output_directory = '../../output-archive/output-fin-2025-06-19'\n",
    "df2_sat = read_dataframe(f'solve_model-satisfiable')\n",
    "df2_dimacs = read_dataframe(f'dimacs')\n",
    "df2_dimacs['year'] = df2_dimacs['committer_date'].apply(lambda d: int(d.year))\n",
    "for df in [df2_sat, df2_dimacs]:\n",
    "    replace_values(df)\n",
    "    df.replace('i386', 'x86', inplace=True)\n",
    "    df.replace('model_to_dimacs_kconfigreader', 'KConfigReader', inplace=True)\n",
    "    df.replace('smt_to_dimacs_z3', 'Z3', inplace=True)\n",
    "    df.replace('sat-competition/', '', inplace=True, regex=True)\n",
    "    df.replace('other/', '', inplace=True, regex=True)\n",
    "    df.replace('.sh', '', inplace=True, regex=True)\n",
    "    df.replace('SAT4J.210', '09-Sat4j 2.1.0', inplace=True, regex=True)\n",
    "    df.replace('SAT4J.231', '11-Sat4j 2.3.1', inplace=True, regex=True)\n",
    "    df.replace('SAT4J.235', '14-Sat4j 2.3.5', inplace=True, regex=True)\n",
    "df2_sat['year-analyzer'] = df2_sat['dimacs-analyzer'].apply(get_year)\n",
    "df2_sat['source-analyzer'] = df2_sat['dimacs-analyzer'].str.split('-', expand=True)[0].str.isdigit().map({True: 'sat-competition', False: 'sat-museum'})\n",
    "df2_sat.loc[df2_sat['dimacs-analyzer'].str.contains('Sat4j'), 'source-analyzer'] = 'FeatureIDE'\n",
    "df2=df2_sat.merge(df2_dimacs).merge(df_features, on=['model-file'], how='left', suffixes=(None, '_2'))\n",
    "pd.set_option('mode.copy_on_write', False)\n",
    "print('unsatisfiable instances: ' + str(len(df2[df2['model-satisfiable']!=True])))\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we determine whether there is a noticeable influence of iterations in our experiment\n",
    "# there isn't for all solvers except 24-kissat-sc2024, so in the following we only plot median values\n",
    "\n",
    "df_iterations = []\n",
    "# for source in ['==2024', 'sat-competition!=2024', 'sat-museum', 'FeatureIDE']:\n",
    "for source in ['==2024', '!=2024']:\n",
    "    pd.set_option('mode.copy_on_write', True)\n",
    "    df_tmp = scope_to(df2.copy(), source)\n",
    "    # df_tmp = df_tmp[df_tmp['model-file'].str.startswith('kconfigreader/1')|df_tmp['model-file'].str.startswith('kmax/1')] # both extractors significantly affect kissat\n",
    "    # df_tmp = df_tmp[df_tmp['dimacs-file'].str.startswith('model_to_dimacs_kconfigreader/1')] # the KConfigReader transformation significantly affects kissat\n",
    "    df_tmp['dimacs-analyzer-time-normalized'] = df_tmp[['year', 'revision', 'architecture', 'extractor', 'dimacs-transformer', 'dimacs-analyzer', 'dimacs-analyzer-time']].groupby(['year', 'revision', 'architecture', 'extractor', 'dimacs-transformer', 'dimacs-analyzer']).transform(lambda x: (x / x.median()))\n",
    "    df_tmp['dimacs-analyzer-time-z-score'] = df_tmp[['year', 'revision', 'architecture', 'extractor', 'dimacs-transformer', 'dimacs-analyzer', 'dimacs-analyzer-time']].groupby(['year', 'revision', 'architecture', 'extractor', 'dimacs-transformer', 'dimacs-analyzer']).transform(lambda x: (x - x.mean()) / x.std())\n",
    "    df_tmp['iteration-source'] = source\n",
    "    df_iterations.append(df_tmp)\n",
    "    pd.set_option('mode.copy_on_write', False)\n",
    "df_iterations = pd.concat(df_iterations)\n",
    "\n",
    "# for y in ['dimacs-analyzer-time-normalized', 'dimacs-analyzer-time-z-score']:\n",
    "for y in ['dimacs-analyzer-time-normalized']:\n",
    "    fig = px.box(\n",
    "    # fig = px.violin(\n",
    "        df_iterations,\n",
    "        # points=False,\n",
    "        # x=df_tmp['year-analyzer'],\n",
    "        facet_col='iteration-source',\n",
    "        color='iteration-source',\n",
    "        y=df_iterations[y],\n",
    "        # Rel. Dev. Median SAT Runtime\n",
    "        labels={'dimacs-analyzer-time-normalized': 'Relative Deviation (log<sub>10</sub>)', 'dimacs-analyzer-time-z-score': 'z-Score For SAT Solving Time', 'architecture': 'Architecture', 'dimacs-analyzer': 'SAT Solver'},\n",
    "        category_orders={'iteration-source': ['==2024', '!=2024']},\n",
    "        hover_data=['revision', 'dimacs-analyzer', 'architecture', 'dimacs-transformer', 'extractor'],\n",
    "        log_y=True,\n",
    "        # points=False\n",
    "    )\n",
    "    fig.update_layout(boxmode='group', boxgap=0.2)\n",
    "    fig.update_traces(width=0.02)\n",
    "    style_box(fig, legend_position=None)\n",
    "    show(fig, 'factor-iteration', width=100, height=0.6*default_height, format='png', scale=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for factor in ['architecture', 'extractor', 'dimacs-transformer', 'source-analyzer']:\n",
    "    print(factor)\n",
    "    df_factor = df2.copy()\n",
    "    if factor == 'architecture':\n",
    "        category_orders = ['x86', 'arm']\n",
    "    elif factor == 'extractor':\n",
    "        category_orders = ['KConfigReader', 'KClause']\n",
    "    elif factor == 'dimacs-transformer':\n",
    "        category_orders = ['KConfigReader', 'Z3']\n",
    "    elif factor == 'source-analyzer':\n",
    "        category_orders = ['sat-competition', 'sat-museum']\n",
    "        # does the gcc compiler version significantly affect results?\n",
    "        df_factor = df_factor[(df_factor['source-analyzer']=='sat-competition') | (df_factor['source-analyzer']=='sat-museum')]\n",
    "        # 2002 and 2003 are different solvers in both datasets\n",
    "        # 2023 and 2024 are not included in the museum dataset\n",
    "        df_factor = df_factor[(df_factor['year-analyzer']!=2002)&(df_factor['year-analyzer']!=2003)&(df_factor['year-analyzer']!=2023)&(df_factor['year-analyzer']!=2024)]\n",
    "\n",
    "    fig = px.box(\n",
    "        df_factor,\n",
    "        y=df_factor['dimacs-analyzer-time'] / 1000000000,\n",
    "        facet_col=factor,\n",
    "        color=factor,\n",
    "        log_y=True,\n",
    "        category_orders={factor: category_orders},\n",
    "        labels={'y': 'SAT Runtime (log<sub>10</sub> s)'},\n",
    "        boxmode='group',\n",
    "        # points=False\n",
    "    )\n",
    "    fig.update_layout(boxmode='group', boxgap=0.2)\n",
    "    fig.update_traces(width=0.02)\n",
    "    style_box(fig, legend_position=None)\n",
    "    show(fig, f'factor-{factor}', width=80, height=0.6*default_height, margin=dict(l=40, r=0, t=0, b=0), format='png', scale=4)\n",
    "    print(scipy.stats.ttest_ind(*df_factor.groupby(factor)['dimacs-analyzer-time'].apply(lambda x:x.values)))\n",
    "    print(cohens_d(*df_factor.groupby(factor)['dimacs-analyzer-time'].apply(lambda x:x.values)))\n",
    "    print(scipy.stats.mannwhitneyu(*df_factor.groupby(factor)['dimacs-analyzer-time'].apply(lambda x:x.values)))\n",
    "    print(effect_size_mannwhitneyu(*df_factor.groupby(factor)['dimacs-analyzer-time'].apply(lambda x:x.values)))\n",
    "    if factor == 'source-analyzer':\n",
    "        fig = px.box(df_factor, x='source-analyzer', y=df_factor['dimacs-analyzer-time'] / 1000000000, facet_col='year-analyzer', log_y=True)\n",
    "        # show(fig, width=4*330, height=default_height)\n",
    "        for year in range(2004, 2023):\n",
    "            if scipy.stats.ttest_ind(*df_factor[(df_factor['year-analyzer']==year)].groupby('source-analyzer')['dimacs-analyzer-time'].apply(lambda x:x.values)).pvalue < 0.005:\n",
    "                print(str(year) + 't: ' + str(cohens_d(*df_factor[(df_factor['year-analyzer']==year)].groupby('source-analyzer')['dimacs-analyzer-time'].apply(lambda x:x.values))))\n",
    "            if scipy.stats.mannwhitneyu(*df_factor[(df_factor['year-analyzer']==year)].groupby('source-analyzer')['dimacs-analyzer-time'].apply(lambda x:x.values)).pvalue < 0.005:\n",
    "                print(str(year) + 'u: ' + str(effect_size_mannwhitneyu(*df_factor[(df_factor['year-analyzer']==year)].groupby('source-analyzer')['dimacs-analyzer-time'].apply(lambda x:x.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, x, legend_position=None, facet_col=None, facet_row=None, color=None, color_discrete_sequence=None, xshift=0, yshift=0, color_value=None, agg='median', remove_architecture=True):\n",
    "    df_tmp = df.copy()\n",
    "    # remove iterations, which almost always have negligible influence\n",
    "    df_tmp = df_tmp.groupby(['year', 'year-analyzer', 'committer_date', 'revision', 'architecture', 'extractor', 'dimacs-transformer', 'dimacs-analyzer']).agg({'dimacs-analyzer-time': agg}).reset_index()\n",
    "    # remove second architecture, which it does not differ significantly\n",
    "    if remove_architecture:\n",
    "        df_tmp = df_tmp[df_tmp['architecture'] == 'x86']\n",
    "    # create additional, optional column for easier plotting\n",
    "    df_tmp['facet'] = df_tmp['extractor'] + ', ' + df_tmp['dimacs-transformer']\n",
    "\n",
    "    df_stats = None\n",
    "    if x == 'committer_date':\n",
    "        fig_ols = px.scatter(\n",
    "            df_tmp,\n",
    "            x=df_tmp[x],\n",
    "            y=df_tmp['dimacs-analyzer-time'] / 1000000000,\n",
    "            facet_col=facet_col,\n",
    "            facet_row=facet_row,\n",
    "            color=color,\n",
    "            symbol='architecture',\n",
    "            log_y=True,\n",
    "            trendline='ols',\n",
    "            trendline_options=dict(log_y=True)\n",
    "        )\n",
    "        results = px.get_trendline_results(fig_ols)\n",
    "        df_stats = []\n",
    "        for i, r in enumerate(results.iloc):\n",
    "            color_value = r[color] if color is not None else None\n",
    "            facet_col_value = r[facet_col] if facet_col is not None else None\n",
    "            facet_row_value = r[facet_row] if facet_row is not None else None\n",
    "            symbol_value = r['architecture']\n",
    "            slope = results.iloc[i]['px_fit_results'].params[1]\n",
    "            yearly = (10**(slope * pd.to_timedelta(1, unit='D').total_seconds() * 365.25))-1\n",
    "            df_row = df_tmp\n",
    "            if color_value is not None:\n",
    "                df_row = df_row[df_row[color] == color_value]\n",
    "            if facet_col_value is not None:\n",
    "                df_row = df_row[df_row[facet_col] == facet_col_value]\n",
    "            if facet_row_value is not None:\n",
    "                df_row = df_row[df_row[facet_row] == facet_row_value]\n",
    "            if symbol_value is not None:\n",
    "                df_row = df_row[df_row['architecture'] == symbol_value]\n",
    "            s = scipy.stats.pearsonr(df_row[x].astype(int) // 10 ** 9, np.log10(df_row['dimacs-analyzer-time'] / 1000000000))\n",
    "            # print(f'{color_value} {facet_col_value} {facet_row_value} {symbol_value}: {yearly:.2%} {s.statistic:.2f} {s.pvalue:.2f}')\n",
    "            df_stats.append({'yearly': yearly, 'r': s.statistic, 'p': s.pvalue, 'color': color_value, 'facet_col': facet_col_value, 'facet_row': facet_row_value, 'symbol': symbol_value})\n",
    "        df_stats = pd.DataFrame(df_stats)\n",
    "\n",
    "    fig = px.line(\n",
    "        df_tmp,\n",
    "        x=df_tmp[x],\n",
    "        y=df_tmp['dimacs-analyzer-time'] / 1000000000,\n",
    "        labels={\n",
    "            'y': 'SAT Runtime (log<sub>10</sub> s)',\n",
    "            'facet': 'Extractor, Transformation',\n",
    "            'dimacs-analyzer': 'SAT Solver',\n",
    "            'architecture': 'Architecture',\n",
    "            'extractor': 'Extractor',\n",
    "            'dimacs-transformer': 'Transformation',\n",
    "            'year-analyzer': 'Year of SAT Solver',\n",
    "            'committer_date': 'Year of Feature Model',\n",
    "            'revision': 'Revision'\n",
    "        },\n",
    "        hover_data=['revision', 'dimacs-analyzer'],\n",
    "        facet_col=facet_col,\n",
    "        facet_row=facet_row,\n",
    "        color=color,\n",
    "        symbol='architecture',\n",
    "        line_dash='architecture',\n",
    "        symbol_sequence=['circle', 'circle-open'],\n",
    "        line_dash_sequence=['solid', 'dot'],\n",
    "        category_orders={\n",
    "            'extractor': ['KConfigReader', 'KClause'],\n",
    "            'dimacs-transformer': ['KConfigReader', 'Z3'],\n",
    "            'architecture': ['x86', 'arm'],\n",
    "            'facet': ['KConfigReader, KConfigReader', 'KConfigReader, Z3', 'KClause, KConfigReader', 'KClause, Z3'],\n",
    "        },\n",
    "        color_discrete_sequence=color_discrete_sequence,\n",
    "        log_y=True,\n",
    "        markers=True\n",
    "    )\n",
    "\n",
    "    fig.update_traces(line_width=1)\n",
    "    style_scatter(fig, legend_position=legend_position)\n",
    "    return fig, df_stats\n",
    "\n",
    "def generate_gradient():\n",
    "    gradient = []\n",
    "    n=8\n",
    "    for i in range(n):\n",
    "        value = int(255 * (((n+1) - i) / (n+1)))\n",
    "        value = int(255 * (i / (n+1)))\n",
    "        color = f'#{value:02x}{value:02x}{value:02x}'\n",
    "        gradient.append(color)\n",
    "    n=10\n",
    "    for i in range(n):\n",
    "        value = int(255 * (((n+1) - i) / (n+1)))\n",
    "        value = int(255 * (i / (n+1)))\n",
    "        color = f'#00{value:02x}ff'\n",
    "        gradient.append(color)\n",
    "    n=5\n",
    "    for i in range(n):\n",
    "        value = int(255 * (((n+1) - i) / (n+1)))\n",
    "        value = int(255 * (i / (n+1)))\n",
    "        # blue_value = int(255 * (1 - (i / n)))\n",
    "        # red_value = int(255 * (i / n))\n",
    "        color = f'#ff{value:02x}00'\n",
    "        gradient.append(color)\n",
    "    return gradient\n",
    "\n",
    "def lreplace(pattern, sub, string):\n",
    "    return re.sub('^%s' % pattern, sub, string)\n",
    "\n",
    "def stats_table(df_stats, group_attributes=['color']):\n",
    "    f = lambda x: lreplace('0\\\\.', '.', x.replace('_', '\\\\_').replace('%', '\\\\%'))\n",
    "    pf = lambda x: f(f'{x:.3f}')\n",
    "    rf = lambda x: f(f'{x:.2f}')\n",
    "    yf = lambda x: f(f'{x:.1%}')\n",
    "    df = df_stats.copy()\n",
    "    df = df.assign(noop='')\n",
    "    if group_attributes is None:\n",
    "        group_attributes = ['noop']\n",
    "    df = df.groupby(group_attributes).agg({'p': ['min', 'median', 'max'], 'r': ['min', 'median', 'max'], 'yearly': ['min', 'median', 'max']}).reset_index()\n",
    "    latex_table = df.to_latex(index=False, formatters=[f, pf, pf, pf, rf, rf, rf, yf, yf, yf])\n",
    "    print(latex_table)\n",
    "\n",
    "def annotate_solvers(fig, df, ay=-15):\n",
    "    fn1 = lambda prefix, label: \"'\" + str(label)[2:4]\n",
    "    last_index = None\n",
    "    last_value = None\n",
    "    for idx, revision in enumerate(df['revision'].unique()):\n",
    "        df_tmp = df[df['revision'] == revision]\n",
    "        current_value = df_tmp['year-analyzer'].iat[0]\n",
    "        height = 1 if idx % 2 == 0 or last_index < idx - 1 else 1.8\n",
    "        if current_value != last_value:\n",
    "            annotate_value(fig, 'committer_date', 'dimacs-analyzer-time', 1, '', 0, ay*height, 'center', df_tmp, fn1, 'year-analyzer', 10)\n",
    "            last_index = idx\n",
    "        last_value = current_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = scope_to(df2, 'sat-museum+competition')\n",
    "fig, df_stats = plot(df_current, x='committer_date', facet_col='extractor', facet_row='dimacs-transformer', color='dimacs-analyzer', color_discrete_sequence=generate_gradient(), legend_position='right', remove_architecture=False)\n",
    "show(fig, width=1500, height=2*default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "fig, _ = plot(df_current, x='committer_date', facet_col='extractor', facet_row='dimacs-transformer', color='dimacs-analyzer', color_discrete_sequence=generate_gradient())\n",
    "show(fig, 'sat', width=450, height=1.2*default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "# print(scipy.stats.pearsonr(df2['committer_date'].astype(int) // 10 ** 9, df2['source_lines_of_code']))\n",
    "# px.box(\n",
    "#     df2,\n",
    "#     x=df2['committer_date'],\n",
    "#     y=df2['dimacs-analyzer-time'] / 1000000000,\n",
    "#     facet_col='extractor',\n",
    "#     facet_row='dimacs-transformer',\n",
    "#     log_y=True\n",
    "# )\n",
    "stats_table(df_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = scope_to(df2, 'sat-competition')\n",
    "fig, df_stats = plot(df_current, x='committer_date', facet_col='extractor', facet_row='dimacs-transformer', color='dimacs-analyzer', color_discrete_sequence=generate_gradient(), legend_position='right', remove_architecture=False)\n",
    "show(fig, width=1500, height=2*default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "df_current = scope_to(df2, 'sat-museum')\n",
    "fig, df_stats = plot(df_current, x='committer_date', facet_col='extractor', facet_row='dimacs-transformer', color='dimacs-analyzer', color_discrete_sequence=generate_gradient(), legend_position='right', remove_architecture=False)\n",
    "show(fig, width=1500, height=2*default_height, margin=dict(l=0, r=0, t=20, b=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = scope_to(df2, 'sat-museum+competition')\n",
    "fig, _ = plot(df_current[df_current['year-analyzer']==2024], x='committer_date', color='facet', agg='max')\n",
    "show(fig, width=250, height=0.7*default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "fig, _ = plot(df_current[df_current['year-analyzer']==2024], x='committer_date', color='facet', agg='min')\n",
    "show(fig, width=250, height=0.7*default_height, margin=dict(l=0, r=0, t=20, b=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = scope_to(df2, 'sat-museum+competition')\n",
    "fig, _ = plot(df_current, x='year-analyzer', facet_col='extractor', facet_row='dimacs-transformer', color='year', color_discrete_sequence=generate_gradient(), legend_position='right', remove_architecture=False)\n",
    "show(fig, width=1500, height=2*default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "fig, _ = plot(df_current, x='year-analyzer', facet_col='extractor', facet_row='dimacs-transformer', color='year', color_discrete_sequence=generate_gradient())\n",
    "# fig.add_vrect(x0=\"2002.5\", x1=\"2003.6\", annotation_text=\"'03\", annotation_position=\"top left\", annotation_textangle=-90, fillcolor=\"gray\", opacity=0.1, line_width=0)\n",
    "# fig.add_vrect(x0=\"2022.5\", x1=\"2023.6\", annotation_text=\"'23\", annotation_position=\"top left\", annotation_textangle=-90, fillcolor=\"gray\", opacity=0.1, line_width=0)\n",
    "# fig.add_vrect(x0=\"2023.5\", x1=\"2024.6\", annotation_text=\"'24\", annotation_position=\"top left\", annotation_textangle=-90, fillcolor=\"gray\", opacity=0.1, line_width=0)\n",
    "show(fig, 'fm', width=460, height=1.2*default_height, margin=dict(l=0, r=10, t=20, b=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = scope_to(df2, 'sat-museum+competition')\n",
    "fig, df_stats = plot(df_current[df_current['year-analyzer'] == df_current['year']], x='committer_date', color='facet', legend_position='right', remove_architecture=False)\n",
    "show(fig, width=700, height=default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "df_current = df_current[df_current['year-analyzer'] == df_current['year']]\n",
    "fig, _ = plot(df_current, x='committer_date', color='facet')\n",
    "fig.update_yaxes(range=[-2.1, 2.6])\n",
    "show(fig, 'equals', width=250, height=0.7*default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "stats_table(df_stats, None)\n",
    "# fig, _ = plot(df_current[df_current['architecture']=='x86'], x='committer_date', color='facet', legend_position='horizontal')\n",
    "# show(fig, 'legend', width=1500, height=default_height, margin=dict(l=0, r=0, t=20, b=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = scope_to(df2, 'sat-museum+competition')\n",
    "df3 = df_current[df_current['year-analyzer'] <= df_current['year']]\n",
    "df3 = df3.loc[df3.groupby(['year', 'committer_date', 'revision', 'architecture', 'extractor', 'dimacs-transformer'])['dimacs-analyzer-time'].idxmin()]\n",
    "fig, df_stats = plot(df3, x='committer_date', color='facet', legend_position='right', remove_architecture=False)\n",
    "show(fig, width=700, height=default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "fig, _ = plot(df3, x='committer_date', color='facet')\n",
    "fig.update_yaxes(range=[-2.1, 2.6])\n",
    "annotate_solvers(fig, df3[(df3['architecture'] == 'x86')&(df3['extractor'] == 'KConfigReader')&(df3['dimacs-transformer'] == 'KConfigReader')])\n",
    "show(fig, 'lessthan', width=250, height=0.7*default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "stats_table(df_stats, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = scope_to(df2, 'sat-museum+competition')\n",
    "df3 = df_current.loc[df_current.groupby(['year', 'committer_date', 'revision', 'architecture', 'extractor', 'dimacs-transformer'])['dimacs-analyzer-time'].idxmin()]\n",
    "fig, df_stats = plot(df3, x='committer_date', color='facet', legend_position='right', remove_architecture=False)\n",
    "show(fig, width=700, height=default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "fig, _ = plot(df3, x='committer_date', color='facet')\n",
    "fig.update_yaxes(range=[-2.1, 2.6])\n",
    "annotate_solvers(fig, df3[(df3['architecture'] == 'x86')&(df3['extractor'] == 'KConfigReader')&(df3['dimacs-transformer'] == 'KConfigReader')])\n",
    "show(fig, 'all', width=250, height=0.7*default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "stats_table(df_stats, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current = scope_to(df2, 'FeatureIDE')\n",
    "fig, df_stats = plot(df_current, x='committer_date', facet_col='extractor', facet_row='dimacs-transformer', color='dimacs-analyzer', color_discrete_sequence=generate_gradient(), legend_position='right', remove_architecture=False)\n",
    "stats_table(df_stats)\n",
    "df_current = pd.concat([\n",
    "    df_current[df_current['year-analyzer'].between(2009, 2010)&df_current['year'].between(2009, 2010)],\n",
    "    df_current[df_current['year-analyzer'].between(2011, 2013)&df_current['year'].between(2011, 2013)],\n",
    "    df_current[(df_current['year-analyzer']>=2014)&(df_current['year']>=2014)]  \n",
    "])\n",
    "show(fig, width=1500, height=2*default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "fig, df_stats = plot(df_current, x='committer_date', color='facet', legend_position='right', remove_architecture=False)\n",
    "show(fig, width=700, height=default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "fig, _ = plot(df_current, x='committer_date', color='facet')\n",
    "fig.update_yaxes(range=[-2.1, 2.6])\n",
    "show(fig, 'featureide', width=250, height=0.7*default_height, margin=dict(l=0, r=0, t=20, b=0))\n",
    "stats_table(df_stats, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_current = scope_to(df2, 'sat-competition')\n",
    "# df_total_features = df_features.groupby(['extractor', 'revision']).agg({'#total_features': 'min'}).reset_index()\n",
    "# df_total_features = pd.merge(df_kconfig[['committer_date', 'revision']].drop_duplicates(), df_total_features)\n",
    "# print(df_total_features)\n",
    "# df_total_features = pd.merge(df_total_features, df_current)\n",
    "# df_total_features = df_total_features[df_total_features['extractor']=='KClause']\n",
    "# df = df_total_features.sort_values(by='committer_date')\n",
    "# df = df[['dimacs-analyzer-time', 'dimacs-analyzer', '#total_features']].drop_duplicates()\n",
    "# # fig = px.scatter(x=df['#total_features'], y=df['dimacs-analyzer-time'], color=df['dimacs-analyzer'], log_y=True)\n",
    "\n",
    "# # style_scatter(fig, marker_size=3, legend_position=None)\n",
    "# # show(fig, width=3*330, height=310)\n",
    "# df_current\n",
    "\n",
    "df_current = scope_to(df2, 'sat-museum+competition')\n",
    "print(\"median SAT solving time: \" + str(df_current['dimacs-analyzer-time'].median()/1000000000))\n",
    "print(\"percentage of SAT queries over one second: \" + str(100-scipy.stats.percentileofscore(df_current['dimacs-analyzer-time']/1000000000, 1)))\n",
    "print(\"percentage of SAT queries over half a second: \" + str(100-scipy.stats.percentileofscore(df_current['dimacs-analyzer-time']/1000000000, 0.5)))\n",
    "\n",
    "print()\n",
    "print(\"median SAT solving per year:\")\n",
    "for year in range(2002, 2025):\n",
    "    print(f\"{year}: \" + str(df_current[df_current['year']==year]['dimacs-analyzer-time'].median()/1000000000))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
